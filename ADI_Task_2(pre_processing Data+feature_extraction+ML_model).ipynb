{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADI_Task#2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo9NngSjnjg0z9slS/pNvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendfekry/my_ml_server/blob/master/ADI_Task_2(pre_processing%20Data%2Bfeature_extraction%2BML_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nDWk3FF86RB",
        "outputId": "73c8fccf-69aa-4f62-9100-d6a303999765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis,KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "c8fwncUvtsPT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/modified_with_text.csv',lineterminator='\\n',encoding='utf-8')\n",
        "\n",
        "X = df.drop(['dialect'],axis=1).values   \n",
        "y = df['dialect'].values\t\t\t\t\n",
        "\n",
        "# Choose test size to split between training and testing sets:\n",
        "x_train, x_te, y_train, y_te = train_test_split(X, y, test_size=.7, random_state=42)\n",
        "\n",
        "\n",
        "# Use the same function above for the validation set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, \n",
        "    test_size=0.2, random_state= 42) \n"
      ],
      "metadata": {
        "id": "OlYetWESk-7E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XhkhL7JmqZSt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_dataframe=pd.DataFrame(y_train)\n",
        "id_dataframe.to_csv('y_train.csv',index=False)\n",
        "id_dataframe=pd.read_csv('y_train.csv')\n",
        "id_dataframe.to_csv(\"y_train.csv\", header=[\"dialect\"], index=False)\n",
        "\n",
        "\n",
        "id_dataframe=pd.DataFrame(y_test)\n",
        "id_dataframe.to_csv('y_test.csv',index=False)\n",
        "id_dataframe=pd.read_csv('y_test.csv')\n",
        "id_dataframe.to_csv(\"y_test.csv\", header=[\"dialect\"], index=False)\n",
        "\n",
        "\n",
        "id_dataframe=pd.DataFrame(x_train)\n",
        "id_dataframe.to_csv('x_train.csv',index=False)\n",
        "id_dataframe2=pd.read_csv('x_train.csv',lineterminator='\\n')\n",
        "id_dataframe.to_csv(\"x_train.csv\", header=[\"id\",\"text\"], index=False)\n",
        "\n",
        "\n",
        "id_dataframe=pd.DataFrame(x_test)\n",
        "id_dataframe.to_csv('x_test.csv',index=False)\n",
        "id_dataframe=pd.read_csv('x_test.csv',lineterminator='\\n')\n",
        "id_dataframe.to_csv(\"x_test.csv\", header=[\"id\",\"text\"], index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dh50UkcnlMaW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('x_test.csv',lineterminator='\\n')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gw8-Mu3wmXek",
        "outputId": "cefa9e9a-eb92-41e3-cd8d-31bbe0bbea82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   id                                               text\n",
              "0  495692014151667712  @AJABreaking اتحس ان الجبير مطرش برشامه حق بوم...\n",
              "1  993436474891698176  @iiiWedad لازم يرفع للي جابه ورازه  في كل مبار...\n",
              "2  900498141350199296  @M_inaa___ بس ترى الكوري مقاسه سمول ماينفع لكم...\n",
              "3  581242125552996352  @zanzounnaaa خرجنا منستاهل وحلال ع قلب الشاطر ...\n",
              "4  751964529333039104  @Lilia_BenChikha ليليا بن شيخة انتي نجمة متالق..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e91acc1e-a56f-429e-8cf3-94dcad90983d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>495692014151667712</td>\n",
              "      <td>@AJABreaking اتحس ان الجبير مطرش برشامه حق بوم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>993436474891698176</td>\n",
              "      <td>@iiiWedad لازم يرفع للي جابه ورازه  في كل مبار...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900498141350199296</td>\n",
              "      <td>@M_inaa___ بس ترى الكوري مقاسه سمول ماينفع لكم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>581242125552996352</td>\n",
              "      <td>@zanzounnaaa خرجنا منستاهل وحلال ع قلب الشاطر ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>751964529333039104</td>\n",
              "      <td>@Lilia_BenChikha ليليا بن شيخة انتي نجمة متالق...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e91acc1e-a56f-429e-8cf3-94dcad90983d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e91acc1e-a56f-429e-8cf3-94dcad90983d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e91acc1e-a56f-429e-8cf3-94dcad90983d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qrF75UcRAlGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d738f9d-f6b5-4269-bfa2-e95b36f506ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('x_train.csv',lineterminator='\\n',encoding='utf-8')\n",
        "test_data=pd.read_csv('x_test.csv',lineterminator='\\n',encoding='utf-8')\n",
        "y_train=pd.read_csv('y_train.csv',lineterminator='\\n',encoding='utf-8')\n",
        "y_test=pd.read_csv('y_test.csv',lineterminator='\\n',encoding='utf-8')\n",
        "\n",
        "train_data=train_data.drop(columns=['id'])\n",
        "test_data=test_data.drop(columns=['id'])\n"
      ],
      "metadata": {
        "id": "fwJNdKPTd1TU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(train_data.text)\n",
        "x_test=np.array(test_data.text)\n",
        "y_train=np.array(y_train.dialect)\n",
        "y_test=np.array(y_test.dialect)"
      ],
      "metadata": {
        "id": "E81kbt_kfLdf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cov_matrix(y_test,pred):\n",
        "  con_mat=confusion_matrix(y_test,pred)\n",
        " # con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "  con_mat_df = pd.DataFrame(con_mat,index =np.unique(y_test), columns = np.unique(y_test))\n",
        "  figure = plt.figure(figsize=(15,15))\n",
        "  sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  print(accuracy_score(y_test,pred))\n",
        "  print(f1_score(y_test,pred, average='macro'))\n",
        "\n",
        "def divide_chunks(l, n): \n",
        "      \n",
        "    # looping till length l \n",
        "    for i in range(0, len(l), n):  \n",
        "        yield l[i:i + n] \n",
        "        "
      ],
      "metadata": {
        "id": "032AAGKbotud"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import strerror\n",
        "english_char=list(string.printable[:-38])\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "\n",
        "\n",
        "def remove_links(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_english(text):\n",
        "    for char in english_char:\n",
        "      \n",
        "      text =re.sub(char, '', text)  \n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    \n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    \n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "        \n",
        "    for word in text.split(\" \"):\n",
        "          if word  in arb_stopwords:  \n",
        "            text=text.replace(word,\"\")\n",
        "    return text\n",
        "\n",
        "def pre_process_document(text):\n",
        "  \n",
        "  text = remove_links(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = remove_stop_words(text)\n",
        "  #text = remove_diacritics(text)\n",
        "  text = remove_repeating_char(text)\n",
        "  text = remove_english(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "pre_x_train=[]  \n",
        "pre_x_test=[]\n",
        "for text in x_train:\n",
        " \n",
        "  pre_x_train.append(pre_process_document(text))\n",
        "for text in x_test:\n",
        "  \n",
        "  pre_x_test.append(pre_process_document(text))\n",
        "pre_x_train=np.array(pre_x_train)\n",
        "pre_x_test=np.array(x_test)"
      ],
      "metadata": {
        "id": "30rPzG0MxZVd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "estimators = [('word', TfidfVectorizer(ngram_range=(1,1),max_df=0.05,min_df=0.0001,stop_words=arb_stopwords)), ('char_b', TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=10000)),('char', TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=10000)) ]\n",
        "combined = FeatureUnion(estimators)\n",
        "x_train_comb = combined.fit_transform(pre_x_train, y_train)\n",
        "x_test_comb=combined.transform(pre_x_test)\n"
      ],
      "metadata": {
        "id": "sg2W0CHQ9HXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7cfd69-344e-4738-a0fe-00ce96a99987"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['آمين', 'أب', 'أخ', 'أفعل', 'أفعله', 'ؤلاء', 'إل', 'إم', 'ات', 'اتان', 'ارتد', 'ان', 'انفك', 'برح', 'تان', 'تبد', 'تحو', 'تعل', 'حد', 'حم', 'حي', 'خب', 'ذار', 'سيما', 'صه', 'ظل', 'ظن', 'عد', 'قط', 'مر', 'مكان', 'مكانكن', 'نب', 'هات', 'هب', 'واها', 'وراء'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(x_train_comb, y_train)\n",
        "\n",
        "linear_pred = linear.predict(x_test)\n",
        "plot_cov_matrix(y_test,linear_pred)\n",
        "accuracy_lin = linear.score(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "iYS6z5UYkKNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(linear,open('svm_model.pkl','wb'))"
      ],
      "metadata": {
        "id": "X7-w9Ur1aOLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}